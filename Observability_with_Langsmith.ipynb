{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVggMBlDsthu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c33727dd-162e-404f-8341-b7d14616ef26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/2.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[90m‚ï∫\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.5/2.5 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/75.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/447.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m447.5/447.5 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install Required Packages\n",
        "!pip install -q langchain langsmith openai python-dotenv\n",
        "!pip install -q langchain-community langchain-openai\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load API Keys from Colab Secrets or Fallback\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')\n",
        "    os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = userdata.get('LANGSMITH_PROJECT')\n",
        "    print(\" API keys loaded from Colab secrets\")\n",
        "except:\n",
        "    os.environ[\"LANGCHAIN_API_KEY\"] = \"your_langsmith_api_key_here\"\n",
        "    os.environ[\"OPENAI_API_KEY\"] = \"your_openai_api_key_here\"\n",
        "    os.environ[\"LANGCHAIN_PROJECT\"] = \"your_project_name_here\"\n",
        "    print(\" Using direct API keys - consider using Colab secrets\")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwvNpu0JKbzp",
        "outputId": "cf539e8e-6bcc-4846-a81b-c68741f3e94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " API keys loaded from Colab secrets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries and Setup LangSmith Project\n",
        "import os, time, random, threading\n",
        "from typing import Dict, Any, List\n",
        "\n",
        "from langsmith import traceable\n",
        "from langsmith.wrappers import wrap_openai\n",
        "from openai import OpenAI\n",
        "\n",
        "os.environ.setdefault(\"LANGSMITH_PROJECT\", os.getenv(\"LANGCHAIN_PROJECT\", \"pr-dependable-headquarters-71\"))\n",
        "\n",
        "openai_client = wrap_openai(OpenAI())\n"
      ],
      "metadata": {
        "id": "WiP6C2_2KdDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Tool with Tracing\n",
        "@traceable(run_type=\"tool\", name=\"web_search_tool\")\n",
        "def web_search(query: str) -> List[str]:\n",
        "    time.sleep(0.05 + random.uniform(0.0, 0.05))\n",
        "    return [f\"Result A for {query}\", f\"Result B for {query}\"]\n"
      ],
      "metadata": {
        "id": "G1lqVzdwKgvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Agent Class with Tracing\n",
        "class SingleAgent:\n",
        "    def __init__(self, name=\"agent-1\", speed=1.0):\n",
        "        self.name = name\n",
        "        self.speed = speed\n",
        "        self.metrics = {\"done\": 0, \"errors\": 0, \"latencies\": []}\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    @traceable(run_type=\"chain\", name=\"SingleAgent.run\")\n",
        "    def run(self, task: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        start = time.time()\n",
        "\n",
        "        docs = web_search(task[\"query\"])\n",
        "\n",
        "        system_msg = \"Answer using only provided results. Be concise.\"\n",
        "        user_q = f\"Q: {task['query']}\\nResults:\\n- \" + \"\\n- \".join(docs)\n",
        "        resp = openai_client.chat.completions.create(\n",
        "            model=\"gpt-4o-mini\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": system_msg},\n",
        "                {\"role\": \"user\", \"content\": user_q},\n",
        "            ],\n",
        "            temperature=0.2,\n",
        "        )\n",
        "\n",
        "        time.sleep((0.05 + random.uniform(0, 0.05)) / self.speed)\n",
        "\n",
        "        ok = random.random() > 0.05\n",
        "        latency = time.time() - start\n",
        "        with self.lock:\n",
        "            if ok:\n",
        "                self.metrics[\"done\"] += 1\n",
        "            else:\n",
        "                self.metrics[\"errors\"] += 1\n",
        "            self.metrics[\"latencies\"].append(latency)\n",
        "\n",
        "        output_content = \"\"\n",
        "        if resp and resp.choices:\n",
        "            output_content = resp.choices[0].message.content\n",
        "\n",
        "        return {\n",
        "            \"agent\": self.name,\n",
        "            \"ok\": ok,\n",
        "            \"latency_s\": round(latency, 3),\n",
        "            \"output\": output_content,\n",
        "        }\n"
      ],
      "metadata": {
        "id": "PLIKOPqGKj2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Agent on Tasks and Print Full Output\n",
        "if __name__ == \"__main__\":\n",
        "    agent = SingleAgent()\n",
        "    tasks = [\n",
        "        {\"id\": i, \"query\": q}\n",
        "        for i, q in enumerate([\n",
        "            \"Best South Indian breakfast options in Bengaluru\",\n",
        "            \"How to implement a thread-pooled agent orchestrator in Python\",\n",
        "            \"Trade-offs of vertical vs horizontal scaling for LLM agents\",\n",
        "            \"Simple KPIs to monitor an AI agent in production\"\n",
        "        ])\n",
        "    ]\n",
        "\n",
        "    results = []\n",
        "    for t in tasks:\n",
        "        out = agent.run(t)\n",
        "        print(f\"\\nTask {t['id']} - ok={out['ok']} latency={out['latency_s']}s\")\n",
        "        print(f\"Output:\\n{out['output']}\")\n",
        "\n",
        "    print(\"\\n Done. Open LangSmith project to inspect:\")\n",
        "    print(f\"Project: {os.getenv('LANGCHAIN_PROJECT', 'default')}\")\n",
        "    print(\"Runs include: SingleAgent.run (parent), web_search_tool (child), OpenAI chat (child).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhPoQhymKmyO",
        "outputId": "ddc77ad5-2aa8-43c8-a21b-9c6e41de164c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Task 0 - ok=True latency=0.956s\n",
            "Output:\n",
            "The best South Indian breakfast options in Bengaluru include those highlighted in Result A and Result B.\n",
            "\n",
            "Task 1 - ok=True latency=12.186s\n",
            "Output:\n",
            "To implement a thread-pooled agent orchestrator in Python, you can use the `concurrent.futures.ThreadPoolExecutor` class. Here‚Äôs a concise outline:\n",
            "\n",
            "1. **Import Required Modules**:\n",
            "   ```python\n",
            "   from concurrent.futures import ThreadPoolExecutor, as_completed\n",
            "   ```\n",
            "\n",
            "2. **Define Your Agent Function**:\n",
            "   Create a function that represents the task each agent will perform.\n",
            "   ```python\n",
            "   def agent_task(agent_id):\n",
            "       # Perform task\n",
            "       return f\"Agent {agent_id} completed\"\n",
            "   ```\n",
            "\n",
            "3. **Set Up the Thread Pool**:\n",
            "   Use `ThreadPoolExecutor` to manage the pool of threads.\n",
            "   ```python\n",
            "   def orchestrator(num_agents):\n",
            "       with ThreadPoolExecutor(max_workers=num_agents) as executor:\n",
            "           futures = {executor.submit(agent_task, i): i for i in range(num_agents)}\n",
            "           for future in as_completed(futures):\n",
            "               agent_id = futures[future]\n",
            "               try:\n",
            "                   result = future.result()\n",
            "                   print(result)\n",
            "               except Exception as exc:\n",
            "                   print(f'Agent {agent_id} generated an exception: {exc}')\n",
            "   ```\n",
            "\n",
            "4. **Run the Orchestrator**:\n",
            "   Call the orchestrator function with the desired number of agents.\n",
            "   ```python\n",
            "   if __name__ == \"__main__\":\n",
            "       orchestrator(5)  # Example with 5 agents\n",
            "   ```\n",
            "\n",
            "This setup allows you to efficiently manage multiple agents running concurrently.\n",
            "\n",
            "Task 2 - ok=True latency=1.481s\n",
            "Output:\n",
            "Vertical scaling involves increasing the resources of a single machine, which can lead to better performance but may hit hardware limits and can be costly. Horizontal scaling distributes the load across multiple machines, offering better fault tolerance and flexibility, but may introduce complexity in management and communication.\n",
            "\n",
            "Task 3 - ok=True latency=1.943s\n",
            "Output:\n",
            "1. Accuracy: Measure the correctness of the AI's predictions.\n",
            "2. Response Time: Track the time taken for the AI to respond to queries.\n",
            "3. Error Rate: Monitor the frequency of incorrect outputs.\n",
            "4. User Engagement: Assess how often users interact with the AI.\n",
            "5. System Uptime: Ensure the AI is operational and available for use.\n",
            "\n",
            "‚úÖ Done. Open LangSmith project to inspect:\n",
            "Project: pr-sparkling-bill-3\n",
            "Runs include: SingleAgent.run (parent), web_search_tool (child), OpenAI chat (child).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç LangSmith AI Agent Observability Demo\n",
        "\n",
        "A comprehensive demonstration of AI agent monitoring and tracing using LangSmith, featuring a single agent with OpenAI integration and tool calling capabilities for production-grade observability.\n",
        "\n",
        "## üåü Features\n",
        "\n",
        "- **üìä Complete Observability**: Full tracing of agent execution with LangSmith integration\n",
        "- **üîß Tool Integration**: Simulated web search tool with performance tracking\n",
        "- **ü§ñ OpenAI Integration**: GPT-4o-mini model with automatic trace capture\n",
        "- **üìà Performance Metrics**: Built-in latency, success rate, and error tracking\n",
        "- **üîí Secure Key Management**: Google Colab secrets integration with fallback options\n",
        "- **üéØ Real-world Tasks**: Practical examples including Bengaluru food recommendations and AI development\n",
        "\n",
        "## üõ†Ô∏è Installation & Setup\n",
        "\n",
        "### Prerequisites\n",
        "\n",
        "The demo requires several Python packages for LangChain, LangSmith, and OpenAI integration:\n",
        "- langchain & langsmith for agent orchestration and tracing\n",
        "- openai for LLM capabilities\n",
        "- python-dotenv for environment management\n",
        "- langchain-community & langchain-openai for extended functionality\n",
        "\n",
        "### API Keys Required\n",
        "\n",
        "You'll need two API keys:\n",
        "1. **LangSmith API Key**: Get from LangSmith settings dashboard\n",
        "2. **OpenAI API Key**: Get from OpenAI platform\n",
        "\n",
        "### Environment Configuration\n",
        "\n",
        "The demo supports multiple configuration methods:\n",
        "\n",
        "**Option 1: Google Colab Secrets (Recommended)**\n",
        "- Store API keys securely in Colab's secret management system\n",
        "- Keys are automatically loaded without exposure in code\n",
        "\n",
        "**Option 2: Direct Environment Variables**\n",
        "- Set LANGSMITH_API_KEY and OPENAI_API_KEY directly\n",
        "- Less secure but useful for local development\n",
        "\n",
        "**Option 3: Direct Assignment**\n",
        "- Fallback method for testing (not recommended for production)\n",
        "\n",
        "## üèóÔ∏è Architecture Overview\n",
        "\n",
        "### Component Structure\n",
        "\n",
        "**SingleAgent Class**\n",
        "- Main agent orchestrator with built-in performance tracking\n",
        "- Thread-safe metrics collection\n",
        "- Configurable execution speed for testing different scenarios\n",
        "\n",
        "**Tool Integration**\n",
        "- Web search tool simulation with realistic latency\n",
        "- Automatic tracing as child runs in LangSmith\n",
        "- Configurable response patterns\n",
        "\n",
        "**OpenAI Integration**\n",
        "- Wrapped OpenAI client for automatic trace capture\n",
        "- Uses GPT-4o-mini for cost-effective processing\n",
        "- System prompts optimized for concise, relevant responses\n",
        "\n",
        "### Tracing Hierarchy\n",
        "\n",
        "The system creates a hierarchical trace structure:\n",
        "1. **Parent Run**: SingleAgent.run (main agent execution)\n",
        "2. **Child Run 1**: web_search_tool (tool execution)\n",
        "3. **Child Run 2**: OpenAI chat completion (LLM call)\n",
        "\n",
        "## üöÄ Demo Workflow\n",
        "\n",
        "### Task Execution Process\n",
        "\n",
        "1. **Tool Invocation**: Agent calls web search tool with user query\n",
        "2. **Result Processing**: Tool returns simulated search results\n",
        "3. **LLM Integration**: Results fed to GPT-4o-mini for answer generation\n",
        "4. **Response Synthesis**: Agent combines tool output with LLM response\n",
        "5. **Metrics Collection**: Performance data captured for analysis\n",
        "\n",
        "### Sample Tasks\n",
        "\n",
        "The demo includes real-world query examples:\n",
        "- **Local Knowledge**: \"Best South Indian breakfast options in Bengaluru\"\n",
        "- **Technical Implementation**: \"How to implement a thread-pooled agent orchestrator in Python\"\n",
        "- **System Design**: \"Trade-offs of vertical vs horizontal scaling for LLM agents\"\n",
        "- **Production Monitoring**: \"Simple KPIs to monitor an AI agent in production\"\n",
        "\n",
        "## üìä Monitoring & Observability\n",
        "\n",
        "### LangSmith Integration\n",
        "\n",
        "**Automatic Tracing**\n",
        "- Every agent execution creates a complete trace\n",
        "- Tool calls and LLM requests automatically captured as child runs\n",
        "- Full request/response logging for debugging\n",
        "\n",
        "**Performance Tracking**\n",
        "- Real-time latency measurements\n",
        "- Success/failure rate monitoring\n",
        "- Error categorization and logging\n",
        "\n",
        "**Visual Dashboard**\n",
        "- LangSmith web interface shows execution flows\n",
        "- Detailed run information including inputs, outputs, and timing\n",
        "- Search and filter capabilities for specific runs or patterns\n",
        "\n",
        "### Built-in Metrics\n",
        "\n",
        "**Execution Metrics**\n",
        "- Task completion count\n",
        "- Error frequency and types\n",
        "- Response time distributions\n",
        "\n",
        "**Quality Metrics**\n",
        "- Success rate tracking (95% target with 5% simulated failures)\n",
        "- Output quality assessment through response analysis\n",
        "\n",
        "## üîß Configuration Options\n",
        "\n",
        "### Agent Parameters\n",
        "\n",
        "**Speed Adjustment**\n",
        "- Configurable execution speed multiplier for testing scenarios\n",
        "- Useful for simulating different load conditions\n",
        "\n",
        "**Error Simulation**\n",
        "- Built-in 5% error rate for realistic testing\n",
        "- Helps validate error handling and recovery mechanisms\n",
        "\n",
        "### LangSmith Project Settings\n",
        "\n",
        "**Project Organization**\n",
        "- Default project: \"agent-observability-demo\"\n",
        "- Configurable via environment variables\n",
        "- Supports multiple project environments\n",
        "\n",
        "**Tracing Configuration**\n",
        "- Full tracing enabled by default\n",
        "- Configurable trace levels and sampling rates\n",
        "\n",
        "## üéØ Use Cases\n",
        "\n",
        "### Development & Testing\n",
        "\n",
        "**Agent Development**\n",
        "- Monitor agent behavior during development\n",
        "- Identify performance bottlenecks\n",
        "- Debug tool integration issues\n",
        "\n",
        "**Load Testing**\n",
        "- Simulate multiple concurrent agent executions\n",
        "- Measure performance under different load conditions\n",
        "- Optimize resource utilization\n",
        "\n",
        "### Production Monitoring\n",
        "\n",
        "**Real-time Observability**\n",
        "- Monitor live agent performance\n",
        "- Track success rates and response times\n",
        "- Alert on anomalous behavior\n",
        "\n",
        "**Quality Assurance**\n",
        "- Verify agent responses meet quality standards\n",
        "- Monitor tool effectiveness\n",
        "- Track user satisfaction metrics\n",
        "\n",
        "## üìà Performance Analysis\n",
        "\n",
        "### Key Metrics Dashboard\n",
        "\n",
        "**Response Time Analysis**\n",
        "- Average, min, max response times\n",
        "- Response time distribution patterns\n",
        "- Performance trends over time\n",
        "\n",
        "**Success Rate Monitoring**\n",
        "- Task completion rates\n",
        "- Error categorization and frequency\n",
        "- Recovery time analysis\n",
        "\n",
        "**Resource Utilization**\n",
        "- API call frequency and costs\n",
        "- Tool usage patterns\n",
        "- System resource consumption\n",
        "\n",
        "### Optimization Insights\n",
        "\n",
        "**Bottleneck Identification**\n",
        "- Slowest components in execution chain\n",
        "- Tool vs LLM performance comparison\n",
        "- Network latency impact analysis\n",
        "\n",
        "**Cost Optimization**\n",
        "- API usage patterns and costs\n",
        "- Token consumption analysis\n",
        "- Efficiency improvement opportunities\n",
        "\n",
        "## üö® Error Handling & Debugging\n",
        "\n",
        "### Built-in Error Simulation\n",
        "\n",
        "The demo includes realistic error scenarios:\n",
        "- Random 5% failure rate for testing resilience\n",
        "- Network timeout simulation\n",
        "- API rate limiting scenarios\n",
        "\n",
        "### Debugging Features\n",
        "\n",
        "**Comprehensive Logging**\n",
        "- Detailed execution logs for each step\n",
        "- Error stack traces and context\n",
        "- Performance timing breakdown\n",
        "\n",
        "**LangSmith Integration**\n",
        "- Visual trace inspection in web interface\n",
        "- Failed run analysis and debugging\n",
        "- Historical error pattern analysis\n",
        "\n",
        "## üîí Security & Best Practices\n",
        "\n",
        "### API Key Management\n",
        "\n",
        "**Secure Storage**\n",
        "- Google Colab secrets integration\n",
        "- Environment variable protection\n",
        "- No hardcoded credentials\n",
        "\n",
        "**Access Control**\n",
        "- Project-based access management\n",
        "- API key rotation capabilities\n",
        "- Audit trail for key usage\n",
        "\n",
        "### Data Privacy\n",
        "\n",
        "**Trace Data Protection**\n",
        "- Configurable data retention policies\n",
        "- PII scrubbing capabilities\n",
        "- Compliance with data protection regulations\n",
        "\n",
        "## üöÄ Deployment Considerations\n",
        "\n",
        "### Local Development\n",
        "\n",
        "**Setup Requirements**\n",
        "- Python environment with required packages\n",
        "- API key configuration\n",
        "- LangSmith account setup\n",
        "\n",
        "**Testing Configuration**\n",
        "- Local project isolation\n",
        "- Development vs production traces\n",
        "- Mock tool integration for offline testing\n",
        "\n",
        "### Production Deployment\n",
        "\n",
        "**Scalability Planning**\n",
        "- Multi-agent orchestration support\n",
        "- Load balancing considerations\n",
        "- Resource allocation optimization\n",
        "\n",
        "**Monitoring Infrastructure**\n",
        "- Integration with existing monitoring systems\n",
        "- Custom alerting rules\n",
        "- Performance baseline establishment\n",
        "\n",
        "## üìö Learning Outcomes\n",
        "\n",
        "### Agent Development Skills\n",
        "\n",
        "**Observability Implementation**\n",
        "- Understanding trace hierarchy design\n",
        "- Performance metric collection strategies\n",
        "- Error handling and recovery patterns\n",
        "\n",
        "**Production Readiness**\n",
        "- Monitoring best practices for AI systems\n",
        "- Quality assurance for agent responses\n",
        "- Cost optimization techniques\n",
        "\n",
        "### LangSmith Expertise\n",
        "\n",
        "**Platform Utilization**\n",
        "- Trace analysis and debugging\n",
        "- Performance optimization insights\n",
        "- Team collaboration features\n",
        "\n",
        "**Integration Patterns**\n",
        "- OpenAI wrapper usage\n",
        "- Custom tool tracing\n",
        "- Multi-component system monitoring\n",
        "\n",
        "## ü§ù Next Steps & Extensions\n",
        "\n",
        "### Enhancement Opportunities\n",
        "\n",
        "**Multi-Agent Support**\n",
        "- Extend to multiple concurrent agents\n",
        "- Agent coordination and communication\n",
        "- Distributed tracing across agent networks\n",
        "\n",
        "**Advanced Tool Integration**\n",
        "- Real API integrations (web search, databases)\n",
        "- Custom tool development and tracing\n",
        "- Tool performance optimization\n",
        "\n",
        "**Production Features**\n",
        "- Advanced error recovery mechanisms\n",
        "- Dynamic configuration management\n",
        "- Automated performance optimization\n",
        "\n",
        "### Community Contributions\n",
        "\n",
        "Areas for contribution and extension:\n",
        "- Additional tool integrations\n",
        "- Enhanced error simulation scenarios\n",
        "- Performance optimization techniques\n",
        "- Documentation improvements\n",
        "\n",
        "## üìû Support & Resources\n",
        "\n",
        "### Getting Help\n",
        "\n",
        "**LangSmith Documentation**\n",
        "- Official platform documentation\n",
        "- Best practices guides\n",
        "- Community forums and support\n",
        "\n",
        "**OpenAI Integration**\n",
        "- API documentation and examples\n",
        "- Cost optimization guides\n",
        "- Model selection recommendations\n",
        "\n",
        "### Community Resources\n",
        "\n",
        "- Sample implementations and extensions\n",
        "- Performance benchmarking results\n",
        "- Integration pattern libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "lXTmx9Z7FARh"
      }
    }
  ]
}